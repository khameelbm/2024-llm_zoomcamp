{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ba1d06-cf2e-4bca-bf4b-e761e67ceffe",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b623b3c7-5249-452f-8d20-21f75c06e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343657e5-8594-4be3-b029-1b7fdb72dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c957189e-41bb-41de-a0f2-5931873aa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = pipeline(task=\"text-classification\",\n",
    "model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb1a8a6-82f1-4c4e-a097-813c6db93732",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Dear seller, I got very impressed with the fast delivery and careful packaging. But the product is broken!\"\n",
    "sentiment = text_classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59778d0f-f825-4fa3-afd2-6e676a2b5790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '2 stars', 'score': 0.4391906261444092}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be65055-b962-410a-918b-049369fbafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 stars\n"
     ]
    }
   ],
   "source": [
    "print(sentiment[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f427d-7a9f-4683-b3ca-8efdb432134b",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a4f23d-b8b3-49cc-88bd-dfa353c25c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "\n",
    "summarizer = pipeline(task = \"summarization\", model = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6271584-abed-4216-88af-332e32c0c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"Design is generally an information dense process involving the creation of a wide variety of formalized documents at different levels of technical complexity. This ranges from those tracking the changes in the requirement analysis stage, conceptual design phase, virtual prototyping process, to the management of the manufacturing processes. Due to this, the idea of AI-aided knowledge distillation and representation support for design has become entrenched in recent years.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe47c39c-9b5d-472f-8993-5d20d82fc191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Design is generally an information dense process involving the creation of a wide variety of formalized documents at different levels of technical complexity. This range'}]\n",
      "Design is generally an information dense process involving the creation of a wide variety of formalized documents at different levels of technical complexity. This range\n"
     ]
    }
   ],
   "source": [
    "output = summarizer(long_text, max_length=30)\n",
    "print(output)\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe409804-f517-40cf-bf36-d60811087861",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for question and answering\n",
    "    - For Q&A, the output of the pipeline is a dictionary, not a list like for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41959e3-3791-42f5-b76b-321dfa1c3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9754096269607544, 'start': 340, 'end': 348, 'answer': '41 years'}\n",
      "41 years\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(task = \"question-answering\")\n",
    "context = \"\"\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). \n",
    "Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\"\"\n",
    "\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "\n",
    "# Pass the inputs to the pipeline\n",
    "outputs = qa_model(question = question, context = context)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs)\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c350f45a-1cbb-458f-8129-e2d6dea38120",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76089ce5-1f13-4f96-92a6-8fa89f9fea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Finite element analysis is known for computational analysis and for statistical analyses (Eckstrom and S. W. Jones 2003). The use of finite elements can be used to demonstrate that mathematical modeling methods may capture all types of data and to test whether assumptions about their structure can be adjusted from natural selection or from the hypothesis theory view, e.g., for each element that has the capacity to be considered one of its constituent parts. Some of the most important finite-element algorithms in recent years have been numerical ones, such as R and Bayesian, R and Bayesian, as well as computational ones, such as a large-scale-scale linear algebra theory based on the FFT. The present paper focuses on the FFT in mathematical modeling of computational models of the distribution functions. It has discussed a wide variety of finite-element algorithms, which have been widely adopted and often used in computational modeling of the natural world, such as the mathematical algorithms discussed in the present paper.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(task = \"text-generation\", model = \"openai-community/gpt2\")\n",
    "\n",
    "prompt = \"Finite element analysis is known for\"\n",
    "\n",
    "output = model(prompt, max_length = 200, truncation=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e68dce-c13c-4668-ab97-d498ca9d585b",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for Translation from Spanish to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5deff2f0-83b6-46be-828e-5cf44411a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course on LLMs is getting very interesting.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import sentencepiece\n",
    "\n",
    "input_text = \"Este curso sobre LLMs se estÃ¡ poniendo muy interesante\"\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline(task = \"translation_es_to_en\", model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0][\"translation_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982362f0-7354-4ad3-9f5b-104a0dcea80b",
   "metadata": {},
   "source": [
    "## Using the transformer pipeline for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5722d741-7700-4874-9f08-998c7a4da3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. \n",
      "The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. The Riverview Experience has exceeded my expectations and I'm glad I was able to enjoy it at my home-based venue. Love that we have other people waiting for us before the reservation is made. This is an extraordinary setting. We'll make the best of it!\n",
      "\n",
      "Hotel was really great! Service was outstanding!! It\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(task =\"text-generation\", model =\"gpt2\")\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n",
    "text = \"\"\"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. \n",
    "The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\"\"\n",
    "\n",
    "# Complete the prompt\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "# Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 150, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "\n",
    "# Print the generated text\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e7900-27b6-46dd-a21d-653154a7589f",
   "metadata": {},
   "source": [
    "## Examining the base transformer architecture using the PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e445e0b-f5e1-4f6f-b5a6-d96a886acb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set transformer model hyperparameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model = d_model,\n",
    "    nhead = n_heads,\n",
    "    num_encoder_layers = num_encoder_layers,\n",
    "    num_decoder_layers = num_decoder_layers\n",
    "          \n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf890fe1-d63f-4003-bc1f-74af723a1d94",
   "metadata": {},
   "source": [
    "## Attention Mechanism - Positional Encoding\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76ce789f-cca9-4fc0-9667-0e7aee42abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math as math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5df27ea-3eb9-4538-943e-7e4e7ca30db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.arange(0,20).unsqueeze(1))\n",
    "\n",
    "# Subclass the appropriate PyTorch class \n",
    "# NOTE: nn.Module class is the base model for all neural network, all nn must subclass this module\n",
    "## use the .register_buffer method to keep parameters untrainable\n",
    "\n",
    "## Initialize a positional encoding matrix for token positions in sequences up to max length\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) \n",
    "\n",
    "  # Update the embeddings tensor adding the positional encodings\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9667660-d175-4c21-844f-9f6c9d1eb380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f375bf6-7b03-4062-871d-21c660e306fc",
   "metadata": {},
   "source": [
    "## Attention mechanism - Multihead attention\n",
    "    - In multi-headed attention, the outputs from different attention heads are combined together and then adjusted to maintain the same size for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b40fa3c-40f2-4607-8024-739f291ac22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the linear transformations for the attention inputs: query, key, and value.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Set the number of attention heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\t\t# Set up the linear transformations\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        # Split the sequence embeddings x across the multiple attention heads.\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split the sequence embeddings in x across the attention heads\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\n",
    "    # Compute dot-product based attention scores between the project query and key.\n",
    "    # Normalize the attention scores to obtain attention weights.\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # This computes attention weights using\n",
    "        # Compute dot-product attention scores\n",
    "        scores = torch.matmul(query, key.permute(1, 2, 0))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # Normalize attention scores into attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    #Multiply the attention weights by the values and linearly transform the concatenated outputs per head.\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\t\t\n",
    "        # Multiply attention weights by values, concatenate and linearly project outputs\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d838e76-b227-4336-9712-c52e82451f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Post-attention feed-forward layer\n",
    "\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92760abd-9248-4ed8-b394-5ce20217ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Assembling a full encoder layer containing:\n",
    "\n",
    "#    A multi-headed self-attention mechanism and feedforward sublayer\n",
    "\n",
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099dedf-5bff-4324-ad28-56d4166a71be",
   "metadata": {},
   "source": [
    "## Encoder transformer body and head\n",
    "\n",
    "  - Implementing the transformer body, namely a stack of multiple encoder layers.\n",
    "  - Appending a task-specific transformer head to process the encoder's resulting hidden states and produce the final outputs for the language task at hand!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9721173-9320-4455-9a51-220d5344a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bc305-4d3f-4abd-a3e7-3194d71ebf03",
   "metadata": {},
   "source": [
    "## Testing the encoder transformer\n",
    "\n",
    "\n",
    "\n",
    "In this exercise, you'll practice creating some instructions to pass an example random sequence throughout the encoder transformer you just defined to obtain and print the classification output. The following variables and model hyperparameters are defined for you:\n",
    "\n",
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1\n",
    "\n",
    "The PositionalEncoder, MultiHeadAttention, FeedForwardSublayer,EncoderLayer, TransformerEncoder, and ClassifierHead classes are also implemented.\n",
    "\n",
    "Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a443c5b-18d5-4db8-9f20-32a9cb849596",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m classifier \u001b[38;5;241m=\u001b[39m ClassifierHead(d_model, num_classes)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Complete the forward pass \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m classification \u001b[38;5;241m=\u001b[39m classifier(output)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification outputs for a batch of \u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 16\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[1;32m     18\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 41\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_linear(key), batch_size)\n\u001b[1;32m     39\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_linear(value), batch_size)\n\u001b[0;32m---> 41\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Multiply attention weights by values, concatenate and linearly project outputs\u001b[39;00m\n\u001b[1;32m     44\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_weights, value)\n",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m, in \u001b[0;36mMultiHeadAttention.compute_attention\u001b[0;34m(self, query, key, mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# This computes attention weights using\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Compute dot-product attention scores\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1e20\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "num_classes = 3 \n",
    "vocab_size = 10000 \n",
    "batch_size = 8 \n",
    "d_model = 512 \n",
    "num_heads = 8 \n",
    "num_layers = 6 \n",
    "d_ff = 2048 \n",
    "sequence_length = 256 \n",
    "dropout = 0.1\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass \n",
    "output = encoder(input_sequence, mask)\n",
    "classification = classifier(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
